{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** : Load, Split and preprocess the imdb dataset. Build, compile, train and evaluate the model with  layer as Embedding , loss function as Binary cross entropy and optimizer as SGD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description** : \n",
    "\n",
    "* Load and split the imdb dataset from tf.keras\n",
    "\n",
    "* Preprocess the dataset with using pad_seqences\n",
    "\n",
    "* Build the model with Sequential API and adding first layer as Embedding with using input parameters as NUM_WORDS as 20000 , DIMENSION as 16  and LEN_WORDS as 300 and add second layer as Global max pooling 1d and adding third layer as dense with 1 neuron and activation function as relu\n",
    "\n",
    "* Compile the model with optimizer as ‘SGD’ , loss as binary cross entropy and metrics as accuracy\n",
    "\n",
    "* Fit the model with training data and epochs as 10\n",
    "* Evaluate the model with testing data as both loss and accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Your Code from Here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries.\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the IMDB datset.\n",
    "\n",
    "\n",
    "NUM_WORDS = 20000\n",
    "(train_data, train_labels),(test_data, test_labels) = # Complete Your Code Here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pad_sequences is used to ensure that all sequences in a list have the same length. By default this is done by padding 0 in the beginning of each sequence until each sequence has the same length as the longest sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding sequences.\n",
    "\n",
    "LEN_WORDS = 300\n",
    "train_data = pad_sequences(# Complete Your Code Here. )\n",
    "test_data = pad_sequences(# Complete Your Code Here. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model.\n",
    "\n",
    "DIMENSION = 16\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(# Complete Your Code Here. ),\n",
    "    tf.keras.layers.GlobalMaxPooling1D(),\n",
    "    tf.keras.layers.Dense(# Complete Your Code Here. )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model.\n",
    "\n",
    "model.fit(# Complete Your Code Here.\n",
    "          verbose=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model.\n",
    "\n",
    "test_loss, test_acc = model.evaluate(# Complete Your Code Here. ,\n",
    "                                     verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the accuracy.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
