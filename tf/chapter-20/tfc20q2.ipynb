{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** : Load, Preprocess and split the Parts Of Speech tagged corpora from NLTK. Use a Pre Trained model as word embedding as GoogleNews-vectors. Build, Compile , Train and Evaluate the model with pre-trained model by layer as RNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Description** :\n",
    "\n",
    "* load POS tagged corpora from NLTK by using brown, treebank and conll2000. Import these three libraries from NLTK corpus\n",
    "\n",
    "* Divide data into words ( X ) and tags ( Y ) using an empty list as  X and Y and store in it.\n",
    "\n",
    "* Convert Text to integer by using text_to_sequences\n",
    "\n",
    "* Truncate long sentences into fixed lengths as 100 \n",
    "\n",
    "* Convert classes to binary form by using to_categorical\n",
    "\n",
    "* Split data into training  and  testing sets ( test set as 0.15 )\n",
    "\n",
    "* Split training data into training and validation sets ( Valid set as 0.15 )\n",
    "\n",
    "* Use a pre-trained model as word embedding as google news vector. load word2vec using the following function present in the       gensim library\n",
    "\n",
    "* assign word vectors from word2vec model and each word in word2vec model is represented using a 300 dimensional vector\n",
    "\n",
    "* create an empty embedding matrix and create a word to index dictionary mapping\n",
    "\n",
    "* copy vectors from word2vec model to the words present in corpus\n",
    "\n",
    "* Build the model by using Sequential API with adding layers as embedding with different dimensions as input_dim     =             VOCABULARY_SIZE,  output_dim    =  EMBEDDING_SIZE,  input_length  =  MAX_SEQ_LENGTH,   weights       = [embedding_weights],     trainable     =  True  and add second layer as SimpleRNN with 64 neurons and return_sequences = True and add third layer as     TimeDistributed layer with also dense layer as NUM_CLASSES and activation function as softmax ( Here Num classes = 13 )\n",
    "\n",
    "* Compile the model with loss as Categorical_crossentropy , optimizer as adam and metrics as accuracy\n",
    "\n",
    "* Fit or train the model with training sets , epochs = 5 , batch_size = 128 and validation sets\n",
    "\n",
    "* Evaluate the model with testing sets as loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Your Code from Here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries.\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import treebank\n",
    "from nltk.corpus import conll2000\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load POS tagged corpora from NLTK\n",
    "\n",
    "treebank_corpus = # Complete Your Code Here.\n",
    "brown_corpus = # Complete Your Code Here.\n",
    "conll_corpus = # Complete Your Code Here.\n",
    "tagged_sentences = treebank_corpus + brown_corpus + conll_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and Y data.\n",
    "\n",
    "X = [] # store input sequence\n",
    "Y = [] # store output sequence\n",
    "\n",
    "for sentence in tagged_sentences:\n",
    "    X_sentence = []\n",
    "    Y_sentence = []\n",
    "    for entity in sentence:         \n",
    "        X_sentence.append(entity[0]) \n",
    "        Y_sentence.append(entity[1]) \n",
    "        \n",
    "    X.append(X_sentence)\n",
    "    Y.append(Y_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all the words lower in X and Y.\n",
    "\n",
    "num_words = len(set([word.lower() for sentence in X for word in sentence]))\n",
    "num_tags   = len(set([word.lower() for sentence in Y for word in sentence]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode X\n",
    "\n",
    "word_tokenizer = # Complete Your Code Here.                    \n",
    "word_tokenizer.fit_on_texts(X)                    \n",
    "X_encoded = # Complete Your Code Here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode Y\n",
    "\n",
    "tag_tokenizer = # Complete Your Code Here.\n",
    "tag_tokenizer.fit_on_texts(Y)\n",
    "Y_encoded = # Complete Your Code Here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different length of X and Y.\n",
    "\n",
    "different_length = [1 if len(inp) != len(output) else 0 for inp, output in zip(X_encoded, Y_encoded)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lenths of sequences in X.\n",
    "\n",
    "lengths = [len(seq) for seq in X_encoded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 100  \n",
    "\n",
    "X_padded = pad_sequences(# Complete Your Code Here. )\n",
    "Y_padded = pad_sequences(# Complete Your Code Here. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = X_padded, Y_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class labels into categorical using to_categorical()\n",
    "\n",
    "Y = to_categorical(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.15\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(# Complete Your Code Here. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "VALID_SIZE = 0.15\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(# Complete Your Code Here. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of tags\n",
    "\n",
    "NUM_CLASSES = Y.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\gupta\\\\Desktop\\\\datasets\\\\GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format(path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE  = 300 \n",
    "VOCABULARY_SIZE = len(word_tokenizer.word_index) + 1\n",
    "\n",
    "\n",
    "embedding_weights = np.zeros((VOCABULARY_SIZE, EMBEDDING_SIZE))\n",
    "\n",
    "word2id = word_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, index in word2id.items():\n",
    "    try:\n",
    "        embedding_weights[index, :] = word2vec[word]\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a model.\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Embedding(# Complete Your Code Here. )\n",
    "\n",
    "model.add(tf.keras.layers.SimpleRNN(# Complete Your Code Here. )\n",
    "\n",
    "model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(# Complete Your Code Here. )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile a model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Train model.\n",
    "\n",
    "model.fit( # Complete Your Code Here. , \n",
    "          validation_data=(X_validation, Y_validation),\n",
    "          verbose=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate a model.\n",
    "\n",
    "loss , accuracy = model.evaluate(# Complete Your Code Here.,\n",
    "                                verbose=0 );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
