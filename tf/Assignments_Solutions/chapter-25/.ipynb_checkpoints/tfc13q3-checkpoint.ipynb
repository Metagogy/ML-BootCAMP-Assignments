{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** : Build, compile, train and evaluate multilayer perceptron with batch Normalization model to address two circles problem using SGD as optimizer and loss as binary categorical cross entropy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description** :\n",
    "\n",
    "* Develop a Multilayer Perceptron with batch Normalization model to address the two circles problem.\n",
    "\n",
    "* X, y = make_circles(n_samples=1000, noise=0.1, random_state=1)\n",
    "\n",
    "* Split the model by using training and testing\n",
    "\n",
    "* Build the model by using the Sequential model and adding the first activation as relu with dense layer having 50 neurons and the second layer has batch normalization and third activation function  as  sigmoid with dense layer having 1 neuron\n",
    "\n",
    "* Compile the model by using SGD optimizer and loss has Categorical_cross entropy\n",
    "\n",
    "* Fit or train the model by using with training, validation data, 100 Epochs, and 0 verbose\n",
    "\n",
    "* Evaluate the model using test set and print the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries.\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "from matplotlib import pyplot\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 2d classification dataset\n",
    "\n",
    "X, y = make_circles(n_samples=1000, noise=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "n_train = 500\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=100, verbose=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8539999723434448\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "\n",
    "_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "print(test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
