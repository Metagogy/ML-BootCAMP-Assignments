{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Predict the stock price of IBM dataset using the 5 layers GRU model without using dropout layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Description**:\n",
    "\n",
    "* The dataset consists from 2006 to 2018 stock price \n",
    "\n",
    "* Split the dataset into training from 2008 to 2016 and testing sets from 2016 to 2017 only\n",
    "\n",
    "* Scale and reshape the training and testing sets\n",
    "\n",
    "* Build the GRU model with 5 layers \n",
    "\n",
    "* Compile the model using mean squared error as loss and Stochastic gradient descent as an optimizer\n",
    "\n",
    "* Train the model using 15 epochs and batch size as 32\n",
    "\n",
    "* Display the root mean squared error loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import GRU, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"C:\\\\Users\\\\gupta\\\\Desktop\\\\datasets\\\\IBM_dataset.csv\",index_col=\"Date\",parse_dates=[\"Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = dataset[:\"2016\"].iloc[:,1:2].values\n",
    "test_set = dataset[\"2017\":].iloc[:,1:2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = MinMaxScaler(feature_range=(0,1))\n",
    "training_set_scaled = sc.fit_transform(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(60,2769):\n",
    "    X_train.append(training_set_scaled[i-60:i,0])\n",
    "    y_train.append(training_set_scaled[i,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train = np.array(X_train),np.array(y_train)\n",
    "\n",
    "X_train = np.reshape(X_train,(X_train.shape[0],X_train.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.GRU(units = 50, return_sequences = True, input_shape = (X_train.shape[1],1), activation = \"tanh\"))\n",
    "model.add(keras.layers.GRU(units = 50, return_sequences = True, activation = \"tanh\"))\n",
    "model.add(keras.layers.GRU(units = 50, return_sequences = True, activation = \"tanh\"))\n",
    "model.add(keras.layers.GRU(units = 50, activation = \"tanh\"))\n",
    "model.add(keras.layers.Dense(units=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = \"mean_squared_error\",optimizer = keras.optimizers.SGD(lr=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train,epochs = 10,batch_size=32,verbose=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_total = pd.concat((dataset[\"High\"][:\"2016\"],dataset[\"High\"][\"2017\":]),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = dataset_total[len(dataset_total)-len(test_set) - 60:].values\n",
    "inputs = inputs.reshape(-1,1)\n",
    "inputs  = sc.transform(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "for i in range(60,311):\n",
    "    X_test.append(inputs[i-60:i,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(X_test)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_stock_price = model.predict(X_test)\n",
    "\n",
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The root mean squared error is 3.563988892518892.\n"
     ]
    }
   ],
   "source": [
    "def return_rmse(test,predicted):\n",
    "    rmse = math.sqrt(mean_squared_error(test, predicted))\n",
    "    print(\"The root mean squared error is {}.\".format(rmse))\n",
    "\n",
    "return_rmse(test_set,predicted_stock_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
