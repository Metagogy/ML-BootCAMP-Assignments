{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** : Load, Split and normalize the mnist dataset with Regularisation. Build, compile, train and evaluate the model using SGD optimizer, loss as \"Sparse_categorical_crossentropy\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description** : \n",
    "\n",
    "Load the mnist dataset using inbuilt dataset function and split into training and testing images.\n",
    "\n",
    "Normalize the dataset images by dividing it with 255 for train and test images. \n",
    "\n",
    "Build the mnist dataset model by defining the function as get_regularised_model(wd,rate) using a sequential layer \n",
    "    \n",
    "    1. Add the first Dense layer with 128 neurons having kernel regularizer as l2 and relu activation with input shape as x_train. \n",
    "    \n",
    "    2. Second layer with dropout  called the argument as step. \n",
    "    \n",
    "    3. The third layer has a Dense layer with 128 neurons and relu activation function and the next layer with dropout called the argument as step. \n",
    "    \n",
    "    4. Repeat the third and fourth layer as 4 times and add next layer has dense with 1 neurons\n",
    "\n",
    "Call the function with momentum as 0.3 and value as 1e-5\n",
    "\n",
    "Compile the model using ‘sgd’ optimizer, loss as \"Sparse_categorical_crossentropy\", metrics as Accuracy and train the model using 3 epochs and validation split as 0.15 and batch size as 32\n",
    "\n",
    "Evaluate the model with testing values. and print the accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOLUTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import silence_tensorflow.auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete get_regularised_model() function.\n",
    "\n",
    "def get_regularised_model(wd,rate):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, kernel_regularizer=regularizers.l2(wd),activation = 'relu',input_shape=(x_train.shape[1:])),\n",
    "        tf.keras.layers.Dropout(rate),\n",
    "        tf.keras.layers.Dense(128, kernel_regularizer=regularizers.l2(wd), activation = 'relu'),\n",
    "        tf.keras.layers.Dropout(rate),\n",
    "        tf.keras.layers.Dense(128, kernel_regularizer=regularizers.l2(wd), activation = 'relu'),\n",
    "        tf.keras.layers.Dropout(rate),\n",
    "        tf.keras.layers.Dense(128, kernel_regularizer=regularizers.l2(wd), activation = 'relu'),\n",
    "        tf.keras.layers.Dropout(rate),\n",
    "        tf.keras.layers.Dense(128, kernel_regularizer=regularizers.l2(wd), activation = 'relu'),\n",
    "        tf.keras.layers.Dropout(rate),\n",
    "        tf.keras.layers.Dense(128, kernel_regularizer=regularizers.l2(wd), activation = 'relu'),\n",
    "        tf.keras.layers.Dropout(rate),\n",
    "        tf.keras.layers.Dense(10,activation='softmax')\n",
    "        \n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model.\n",
    "model = get_regularised_model(1e-5, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model.\n",
    "\n",
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer='SGD',\n",
    "             metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model.\n",
    "\n",
    "model.fit(x_train, y_train, epochs=3,batch_size=32,verbose=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9258000254631042\n"
     ]
    }
   ],
   "source": [
    "# Evaluate and print the accuracy.\n",
    "\n",
    "history=model.evaluate(x_test, y_test, verbose = 0)\n",
    "\n",
    "print(history[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
