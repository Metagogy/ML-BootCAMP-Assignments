{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** : Load, Preprocess and split the Parts Of Speech tagged corpora from NLTK. Build model with layer as RNN and print the summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Description** :\n",
    "\n",
    "load POS tagged corpora from NLTK by using brown, treebank and conll2000. Import these three libraries from NLTK corpus\n",
    "\n",
    "Divide data into words ( X ) and tags ( Y ) using an empty list as  X and Y and store in it.\n",
    "\n",
    "Convert Text to integer by using text_to_sequences\n",
    "\n",
    "Truncate long sentences into fixed lengths as 100 \n",
    "\n",
    "Convert classes to binary form by using to_categorical\n",
    "\n",
    "Split data into training  and  testing sets ( test set as 0.15 )\n",
    "\n",
    "Split training data into training and validation sets ( Valid set as 0.15 )\n",
    "\n",
    "Initialize EMBEDDING_SIZE  = 300  and VOCABULARY_SIZE = len(word_tokenizer.word_index) + 1 , Here word_tokenizer = Tokenizer()\n",
    "\n",
    "Build the model by using Sequential API with adding layers as embedding with different dimensions as input_dim     =  VOCABULARY_SIZE,  output_dim    =  EMBEDDING_SIZE,  input_length  =  MAX_SEQ_LENGTH,   trainable     =  False   and add second layer as SimpleRNN with 64 neurons and return_sequences = True and add third layer as TimeDistributed layer with also dense layer as NUM_CLASSES and activation function as softmax ( Here Num classes = 13 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Level**: Medium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input Format** : \n",
    "POS tagged corpora from NLTK\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output Format** : \n",
    "Neural network model summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample Input** : \n",
    "Load the dataset using libraries called brown, treebank , conll2000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample Output** : \n",
    "Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "from nltk.corpus import treebank\n",
    "from nltk.corpus import conll2000\n",
    "\n",
    "# load POS tagged corpora from NLTK\n",
    "treebank_corpus = treebank.tagged_sents(tagset='universal')\n",
    "brown_corpus = brown.tagged_sents(tagset='universal')\n",
    "conll_corpus = conll2000.tagged_sents(tagset='universal')\n",
    "tagged_sentences = treebank_corpus + brown_corpus + conll_corpus\n",
    "\n",
    "# let's look at the data\n",
    "tagged_sentences[2]\n",
    "\n",
    "X = [] # store input sequence\n",
    "Y = [] # store output sequence\n",
    "\n",
    "for sentence in tagged_sentences:\n",
    "    X_sentence = []\n",
    "    Y_sentence = []\n",
    "    for entity in sentence:         \n",
    "        X_sentence.append(entity[0]) \n",
    "        Y_sentence.append(entity[1]) \n",
    "        \n",
    "    X.append(X_sentence)\n",
    "    Y.append(Y_sentence)\n",
    "\n",
    "X[1]\n",
    "\n",
    "num_words = len(set([word.lower() for sentence in X for word in sentence]))\n",
    "num_tags   = len(set([word.lower() for sentence in Y for word in sentence]))\n",
    "print(\"Total number of tagged sentences: {}\".format(len(X)))\n",
    "print(\"Vocabulary size: {}\".format(num_words))\n",
    "print(\"Total number of tags: {}\".format(num_tags))\n",
    "\n",
    "print('sample X: ', X[0], '\\n')\n",
    "print('sample Y: ', Y[0], '\\n')\n",
    "\n",
    "print(\"Length of first input sequence  : {}\".format(len(X[0])))\n",
    "print(\"Length of first output sequence : {}\".format(len(Y[0])))\n",
    "\n",
    "X[1]\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# encode X\n",
    "\n",
    "word_tokenizer = Tokenizer()                      word_tokenizer.fit_on_texts(X)                    \n",
    "X_encoded = word_tokenizer.texts_to_sequences(X)  \n",
    "\n",
    "# encode Y\n",
    "\n",
    "tag_tokenizer = Tokenizer()\n",
    "tag_tokenizer.fit_on_texts(Y)\n",
    "Y_encoded = tag_tokenizer.texts_to_sequences(Y)\n",
    "\n",
    "print(\"** Raw data point **\", \"\\n\", \"-\"*100, \"\\n\")\n",
    "print('X: ', X[0], '\\n')\n",
    "print('Y: ', Y[0], '\\n')\n",
    "print()\n",
    "print(\"** Encoded data point **\", \"\\n\", \"-\"*100, \"\\n\")\n",
    "print('X: ', X_encoded[0], '\\n')\n",
    "print('Y: ', Y_encoded[0], '\\n')\n",
    "\n",
    "different_length = [1 if len(input) != len(output) else 0 for input, output in zip(X_encoded, Y_encoded)]\n",
    "print(\"{} sentences have disparate input-output lengths.\".format(sum(different_length)))\n",
    "\n",
    "\n",
    "lengths = [len(seq) for seq in X_encoded]\n",
    "print(\"Length of longest sentence: {}\".format(max(lengths)))\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "sns.boxplot(lengths)\n",
    "plt.show()\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_SEQ_LENGTH = 100  \n",
    "X_padded = pad_sequences(X_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")\n",
    "Y_padded = pad_sequences(Y_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")\n",
    "\n",
    "# print the first sequence\n",
    "print(X_padded[0], \"\\n\"*3)\n",
    "print(Y_padded[0])\n",
    "\n",
    "\n",
    "X, Y = X_padded, Y_padded\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "Y = to_categorical(Y)\n",
    "Y.shape\n",
    "\n",
    "X\n",
    "X.shape\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TEST_SIZE = 0.15\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=TEST_SIZE, random_state=4)\n",
    "X_train.shape\n",
    "\n",
    "VALID_SIZE = 0.15\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X_train, Y_train, test_size=VALID_SIZE, random_state=4)\n",
    "\n",
    "# print number of samples in each set\n",
    "print(\"TRAINING DATA\")\n",
    "print('Shape of input sequences: {}'.format(X_train.shape))\n",
    "print('Shape of output sequences: {}'.format(Y_train.shape))\n",
    "print(\"-\"*50)\n",
    "print(\"VALIDATION DATA\")\n",
    "print('Shape of input sequences: {}'.format(X_validation.shape))\n",
    "print('Shape of output sequences: {}'.format(Y_validation.shape))\n",
    "print(\"-\"*50)\n",
    "print(\"TESTING DATA\")\n",
    "print('Shape of input sequences: {}'.format(X_test.shape))\n",
    "print('Shape of output sequences: {}'.format(Y_test.shape))\n",
    "\n",
    "NUM_CLASSES = Y.shape[2]\n",
    "\n",
    "NUM_CLASSES\n",
    "import tensorflow as tf\n",
    "\n",
    "EMBEDDING_SIZE  = 300  \n",
    "VOCABULARY_SIZE = len(word_tokenizer.word_index) + 1\n",
    "\n",
    "# create architecture\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Embedding(input_dim     =  VOCABULARY_SIZE,        output_dim    =  EMBEDDING_SIZE,                                 input_length  =  MAX_SEQ_LENGTH, trainable =  False))\n",
    "\n",
    "model.add(tf.keras.layers.SimpleRNN(64, \n",
    "              return_sequences=True ))\n",
    "\n",
    "model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')))\n",
    "\n",
    "model.summary()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
