{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** : Load, Split and preprocess the imdb dataset. Build, compile and train the model with  layer as Embedding , loss function as Binary cross entropy and optimizer as SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description** : \n",
    "\n",
    "Load and split the imdb dataset from tf.keras\n",
    "\n",
    "Preprocess the dataset with using pad_seqences\n",
    "\n",
    "Build the model with Sequential API and adding first layer as Embedding with using input parameters as NUM_WORDS as 20000 , \n",
    "\n",
    "DIMENSION as 16  and LEN_WORDS as 300 and add second layer as Global average pooling 1d and adding third layer as dense with 1 \n",
    "neuron and activation function as sigmoid\n",
    "\n",
    "Compile the model with optimizer as ‘SGD’ , loss as binary cross entropy and metrics as accuracy\n",
    "\n",
    "Fit the model with training data and epochs as 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Level:** Hard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input format** : \n",
    "Imdb dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output format** : \n",
    "Neural network model accuracy and loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample input** : \n",
    "Load the dataset using inbuilt function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample Output** : \n",
    "Accuracy  and loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOLUTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import preprocessing, models, layers, datasets\n",
    "\n",
    "imdb = datasets.imdb\n",
    "NUM_WORDS = 20000\n",
    "(train_data, train_labels),(test_data, test_labels) = imdb.load_data(num_words = NUM_WORDS,)\n",
    "\n",
    "LEN_WORDS = 300\n",
    "train_data = preprocessing.sequence.pad_sequences(train_data, maxlen=LEN_WORDS)\n",
    "test_data = preprocessing.sequence.pad_sequences(test_data , maxlen = LEN_WORDS)\n",
    "\n",
    "test_data.shape\n",
    "\n",
    "DIMENSION = 16\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(NUM_WORDS, DIMENSION, input_length=LEN_WORDS),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(optimizer='SGD', loss=tf.keras.losses.binary_crossentropy, metrics=['accuracy'])\n",
    "model.fit(train_data, train_labels, epochs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
