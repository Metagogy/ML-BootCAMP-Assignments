{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** : Load, Split and preprocess the imdb dataset. Build, compile, train and evaluate the model with  layer as separable conv1d , loss function as Binary cross entropy and optimizer as RMSprop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Description** : \n",
    "\n",
    "Load and split the imdb dataset from tf.keras\n",
    "\n",
    "Preprocess the dataset with using pad_seqences\n",
    "\n",
    "Build the model with Sequential API and adding first layer as Embedding with using input parameters as NUM_WORDS as 20000 , DIMENSION as 16  and LEN_WORDS as 300 and add second layer as separable convolution 1d with filter as 10, kernel_size as 3, strides as 3 and padding as same and adding third layer as global average pooling 1d and adding fourth layer as dense with 1 neuron and activation function as relu\n",
    "\n",
    "Compile the model with optimizer as ‘RMSprop’ , loss as binary cross entropy and metrics as accuracy\n",
    "\n",
    "Fit the model with training data and epochs as 5\n",
    "\n",
    "Evaluate the model with testing data as both loss and accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Level:** Hard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input format** : \n",
    "Imdb dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output format** : \n",
    "Neural network model accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample input** : \n",
    "Load the dataset using inbuilt function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample Output** : \n",
    "Accuracy \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import preprocessing, models, layers, datasets\n",
    "\n",
    "imdb = datasets.imdb\n",
    "NUM_WORDS = 20000\n",
    "(train_data, train_labels),(test_data, test_labels) = imdb.load_data(num_words = NUM_WORDS,)\n",
    "\n",
    "LEN_WORDS = 300\n",
    "train_data = preprocessing.sequence.pad_sequences(train_data, maxlen=LEN_WORDS)\n",
    "test_data = preprocessing.sequence.pad_sequences(test_data , maxlen = LEN_WORDS)\n",
    "\n",
    "test_data.shape\n",
    "\n",
    "DIMENSION = 16\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Embedding(NUM_WORDS, DIMENSION, input_length=LEN_WORDS),\n",
    "    layers.SeparableConv1D(filters=10, kernel_size=3, strides=3, padding='same'),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dense(1, activation='relu')\n",
    "])\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(optimizer='RMSprop', loss=tf.keras.losses.binary_crossentropy , metrics=['accuracy'])\n",
    "model.fit(train_data, train_labels, epochs=5)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_data, test_labels)\n",
    "print('Test loss: {}'.format(test_loss))\n",
    "print('Test accuracy: {}'.format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
