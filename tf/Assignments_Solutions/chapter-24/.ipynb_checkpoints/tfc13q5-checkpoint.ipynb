{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** : Load and normalize the mnist dataset. Evaluate the model using SGD optimizer, loss as \"categorical_crossentropy\" with layer as Dropout.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description** : \n",
    "\n",
    "* Load the mnist dataset using inbuilt dataset function and split into training and testing images.\n",
    "\n",
    "* Flatten the dataset images by using reshape. \n",
    "\n",
    "* Normalize the dataset images by dividing it with 255 \n",
    "\n",
    "* Transform class labels into binary class matrices\n",
    "\n",
    "* Build the mnist dataset model by using a sequential layer and adding the first layer as dense with 128 neurons and input_shape as 784 and next activation functions as Relu. Add third layers having 0.3 neurons in dropout layer and fourth layer having 128 neurons in dense layer and next activation function as relu. Add a sixth layer having 0.3 neurons in the dropout layer and the next layer as Dense with 10 neurons and the next activation function as Softmax.\n",
    "\n",
    "* Compile the model using SGD optimizer, loss as \"categorical_crossentropy\" \n",
    "\n",
    "* Train the model using 10 epochs, 128 as batch size, 1 as verbose and validation_split as 0.2\n",
    "\n",
    "* Evaluate the mnist model by using test images and labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOLUTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries.\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mnist dataset.\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten or reshape images.\n",
    "\n",
    "RESHAPED = 784\n",
    "X_train = x_train.reshape(60000, RESHAPED)\n",
    "X_test = x_test.reshape(10000, RESHAPED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert class labels into categorical values.\n",
    "\n",
    "y_train=to_categorical(y_train)\n",
    "y_test=to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert train and test data into float data type.\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data.\n",
    "\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Sequential model.\n",
    "\n",
    "model= tf.keras.Sequential()\n",
    "model.add (tf.keras.layers.Dense(128, input_shape = (784,)))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "model.add (tf.keras.layers.Dense(128))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "model.add (tf.keras.layers.Dense(10))\n",
    "model.add(tf.keras.layers.Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model.\n",
    "\n",
    "model.compile(loss = tf.keras.losses.categorical_crossentropy,\n",
    "             optimizer = 'SGD',\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model.\n",
    "\n",
    "history = mo` del.fit(X_train, y_train,\n",
    "                   batch_size = 128, \n",
    "                   epochs = 10,  \n",
    "                   verbose = 0,\n",
    "                   validation_split = 0.2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9243999719619751\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model and print the accuracy.\n",
    "\n",
    "score = model.evaluate (X_test, y_test, verbose = 0)\n",
    "\n",
    "print(score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
