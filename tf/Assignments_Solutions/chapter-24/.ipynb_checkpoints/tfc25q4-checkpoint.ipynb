{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** : Load, Split and preprocess the imdb dataset. Build, compile, train and evaluate the model with layer as separable conv1d , loss function as Binary cross entropy and optimizer as adam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description** : \n",
    "\n",
    "* Load and split the imdb dataset from tf.keras\n",
    "\n",
    "* Preprocess the dataset with using pad_seqences\n",
    "\n",
    "* Build the model with Sequential API and adding first layer as Embedding with using input parameters as NUM_WORDS as 20000 , DIMENSION as 16  and LEN_WORDS as 300 and add second layer as separable convolution 1d with filter as 10, kernel_size as 3, strides as 3 and padding as same and adding third layer as global average pooling 1d and adding fourth layer as dense with 1 neuron and activation function as sigmoid\n",
    "\n",
    "* Compile the model with optimizer as ‘Adam’ , loss as binary cross entropy and metrics as accuracy\n",
    "\n",
    "* Fit the model with training data and epochs as 10\n",
    "\n",
    "* Evaluate the model with testing data as both loss and accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOLUTION** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import preprocessing, models, layers, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = datasets.imdb\n",
    "NUM_WORDS = 20000\n",
    "(train_data, train_labels),(test_data, test_labels) = imdb.load_data(num_words = NUM_WORDS,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEN_WORDS = 300\n",
    "train_data = preprocessing.sequence.pad_sequences(train_data, maxlen=LEN_WORDS)\n",
    "test_data = preprocessing.sequence.pad_sequences(test_data , maxlen = LEN_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIMENSION = 16\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Embedding(NUM_WORDS, DIMENSION, input_length=LEN_WORDS),\n",
    "    layers.SeparableConv1D(filters=10, kernel_size=3, strides=3, padding='same'),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=tf.keras.losses.binary_crossentropy , metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_data, train_labels, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
