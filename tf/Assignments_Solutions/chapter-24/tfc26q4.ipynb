{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Build, compile, train, and evaluate sarcasm dataset using the sequential model with two layers LSTM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description:**\n",
    "\n",
    "* Load the sarcasm JSON file using pandas method and select only headline, is_sarcastic features\n",
    "\n",
    "* Using regular expression replace special symbols and digits in headline feature\n",
    "\n",
    "* Split the data set into training and testing sets with 20,000 and 6709 rows respectively\n",
    "\n",
    "* Preprocess the dataset using tokenizer by including vocabulary size as 1000, post padding, and maximum sentence length as 120\n",
    "\n",
    "* Build the model with Sequential API and add the first layer as Embedding with using input parameters as NUM_WORDS as 1000, DIMENSION as 16  and LEN_WORDS as 120 and add the second layer as LSTM with 32 neurons, third layer as LSTM with 64 neurons, fourth dense layer as 24 neurons with ‘relu’ activation function and fifth layer as dense with 1 neuron with activation function as sigmoid\n",
    "\n",
    "* Compile the model with optimizer as ‘Adam’, loss as binary cross-entropy, and metrics as accuracy\n",
    "\n",
    "* Convert the dataset into list format to array using numpy\n",
    "\n",
    "* Fit the model with training data and epochs as 5\n",
    "\n",
    "* Evaluate the model with testing data and print accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries.\n",
    "\n",
    "import json\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sarcasm data.\n",
    "\n",
    "data = pd.read_json('/home/metagogy/sarcasm.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data and Create sentences and labels.\n",
    "\n",
    "data['headline'] = data['headline'].apply(lambda s : re.sub('[^a-zA-Z]', ' ', s))\n",
    "\n",
    "sentences = data['headline']\n",
    "labels = data['is_sarcastic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test data.\n",
    "\n",
    "training_sentences = sentences[0:20000]\n",
    "testing_sentences = sentences[20000:]\n",
    "training_labels = labels[0:20000]\n",
    "testing_labels = labels[20000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenizer on sequences and also apply pad_sequences to zero padding the sequences. \n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "\n",
    "training_padded = pad_sequences(training_sequences, maxlen=120, padding='post', truncating='post')\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=120, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a model.\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(1000, 16, input_length=120),\n",
    "    tf.keras.layers.LSTM(32, return_sequences = True),\n",
    "    tf.keras.layers.LSTM(64),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile a model.\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert train and test data into array.\n",
    "\n",
    "training_padded = np.array(training_padded)\n",
    "training_labels = np.array(training_labels)\n",
    "testing_padded = np.array(testing_padded)\n",
    "testing_labels = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model.\n",
    "\n",
    "model.fit(training_padded, training_labels, epochs=5, validation_data=(testing_padded,testing_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model.\n",
    "\n",
    "test_loss, test_acc = model.evaluate(testing_padded,testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the accuracy.\n",
    "\n",
    "print(test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
