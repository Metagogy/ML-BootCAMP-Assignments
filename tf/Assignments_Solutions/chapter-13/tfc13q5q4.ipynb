{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** : Load and normalize the mnist dataset. Train the model using with Epochs and batchsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description** : \n",
    "\n",
    "Load the mnist dataset using inbuilt dataset function and split into training and testing images.\n",
    "\n",
    "Flatten the dataset images by using reshape. \n",
    "\n",
    "Normalize the dataset images by dividing it with 255 \n",
    "\n",
    "Transform class labels into binary class matrices\n",
    "\n",
    "Build the mnist dataset model by using a sequential layer and adding the first layer as dense with 128 neurons and input_shape as 784 and next activation functions as Relu. Add third layers having 0.3 neurons in dropout layer and fourth layer having 128 neurons in dense layer and next activation function as relu. Add a sixth layer having 0.3 neurons in the dropout layer and the next layer as Dense with 10 neurons and the next activation function as Softmax.\n",
    "\n",
    "Compile the model using SGD optimizer, loss as \"categorical_crossentropy\" \n",
    "\n",
    "Train the model using 10 epochs, 128 as batch size, 1 as verbose and validation_split as 0.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Level** : Hard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input format** : Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output format** : Accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample input** : Load the dataset using inbuilt function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample Output** : Model accuracy for training set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOLUTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "RESHAPED = 784\n",
    "X_train = x_train.reshape(60000, RESHAPED)\n",
    "X_test = x_test.reshape(10000, RESHAPED)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "Y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "Y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add (tf.keras.layers.Dense(128, input_shape = (784,)))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "model.add (tf.keras.layers.Dense(128))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "model.add (tf.keras.layers.Dense(10))\n",
    "model.add(tf.keras.layers.Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss = tf.keras.losses.categorical_crossentropy,\n",
    "             optimizer = 'SGD',\n",
    "             metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train,\n",
    "                   batch_size = 128, \n",
    "                   epochs = 10,  \n",
    "                   verbose = 1,\n",
    "                   validation_split = 0.2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
