{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** : Load, Preprocess and split the Parts Of Speech tagged corpora from NLTK. Use a Pre Trained model as word embedding as GoogleNews-vectors. Build the model with pre-trained model by layer as LSTM and print the summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description** :\n",
    "\n",
    "load POS tagged corpora from NLTK by using brown, treebank and conll2000. Import these three libraries from NLTK corpus\n",
    "\n",
    "Divide data into words ( X ) and tags ( Y ) using an empty list as  X and Y and store in it.\n",
    "\n",
    "Convert Text to integer by using text_to_sequences\n",
    "\n",
    "Truncate long sentences into fixed lengths as 100 \n",
    "\n",
    "Convert classes to binary form by using to_categorical\n",
    "\n",
    "Split data into training  and  testing sets ( test set as 0.15 )\n",
    "\n",
    "Split training data into training and validation sets ( Valid set as 0.15 )\n",
    "\n",
    "Use a pre-trained model as word embedding as google news vector. load word2vec using the following function present in the gensim library\n",
    "\n",
    "assign word vectors from word2vec model and each word in word2vec model is represented using a 300 dimensional vector\n",
    "\n",
    "create an empty embedding matrix and create a word to index dictionary mapping\n",
    "\n",
    "copy vectors from word2vec model to the words present in corpus\n",
    "\n",
    "Build the model by using Sequential API with adding layers as embedding with different dimensions as input_dim     =  VOCABULARY_SIZE,  output_dim    =  EMBEDDING_SIZE,  input_length  =  MAX_SEQ_LENGTH,   weights       = [embedding_weights], trainable     =  True  and add second layer as LSTM with 64 neurons and return_sequences = True and add third layer as TimeDistributed layer with also dense layer as NUM_CLASSES and activation function as softmax ( Here Num classes = 13 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Level**: Hard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input Format** : \n",
    "POS tagged corpora from NLTK and Word2Vec model from Google News vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output Format** : \n",
    "Neural network model summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample Input** : \n",
    "Load the dataset using libraries called brown, treebank , conll2000 and load word2vec using the following function present in the gensim library\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample Output** : \n",
    "Model summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "from nltk.corpus import treebank\n",
    "from nltk.corpus import conll2000\n",
    "\n",
    "# load POS tagged corpora from NLTK\n",
    "treebank_corpus = treebank.tagged_sents(tagset='universal')\n",
    "brown_corpus = brown.tagged_sents(tagset='universal')\n",
    "conll_corpus = conll2000.tagged_sents(tagset='universal')\n",
    "tagged_sentences = treebank_corpus + brown_corpus + conll_corpus\n",
    "\n",
    "# let's look at the data\n",
    "tagged_sentences[2]\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for sentence in tagged_sentences:\n",
    "    X_sentence = []\n",
    "    Y_sentence = []\n",
    "    for entity in sentence:         \n",
    "        X_sentence.append(entity[0]) \n",
    "        Y_sentence.append(entity[1]) \n",
    "        \n",
    "    X.append(X_sentence)\n",
    "    Y.append(Y_sentence)\n",
    "\n",
    "X[1]\n",
    "\n",
    "num_words = len(set([word.lower() for sentence in X for word in sentence]))\n",
    "num_tags   = len(set([word.lower() for sentence in Y for word in sentence]))\n",
    "print(\"Total number of tagged sentences: {}\".format(len(X)))\n",
    "print(\"Vocabulary size: {}\".format(num_words))\n",
    "print(\"Total number of tags: {}\".format(num_tags))\n",
    "\n",
    "print('sample X: ', X[0], '\\n')\n",
    "print('sample Y: ', Y[0], '\\n')\n",
    "\n",
    "print(\"Length of first input sequence  : {}\".format(len(X[0])))\n",
    "print(\"Length of first output sequence : {}\".format(len(Y[0])))\n",
    "\n",
    "X[1]\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# encode X\n",
    "\n",
    "word_tokenizer = Tokenizer()                     word_tokenizer.fit_on_texts(X)                    \n",
    "X_encoded = word_tokenizer.texts_to_sequences(X)  \n",
    "\n",
    "# encode Y\n",
    "\n",
    "tag_tokenizer = Tokenizer()\n",
    "tag_tokenizer.fit_on_texts(Y)\n",
    "Y_encoded = tag_tokenizer.texts_to_sequences(Y)\n",
    "\n",
    "# look at first encoded data point\n",
    "\n",
    "print(\"** Raw data point **\", \"\\n\", \"-\"*100, \"\\n\")\n",
    "print('X: ', X[0], '\\n')\n",
    "print('Y: ', Y[0], '\\n')\n",
    "print()\n",
    "print(\"** Encoded data point **\", \"\\n\", \"-\"*100, \"\\n\")\n",
    "print('X: ', X_encoded[0], '\\n')\n",
    "print('Y: ', Y_encoded[0], '\\n')\n",
    "\n",
    "different_length = [1 if len(input) != len(output) else 0 for input, output in zip(X_encoded, Y_encoded)]\n",
    "print(\"{} sentences have disparate input-output lengths.\".format(sum(different_length)))\n",
    "\n",
    "lengths = [len(seq) for seq in X_encoded]\n",
    "print(\"Length of longest sentence: {}\".format(max(lengths)))\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "sns.boxplot(lengths)\n",
    "plt.show()\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_SEQ_LENGTH = 100  \n",
    "X_padded = pad_sequences(X_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")\n",
    "Y_padded = pad_sequences(Y_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")\n",
    "\n",
    "print(X_padded[0], \"\\n\"*3)\n",
    "print(Y_padded[0])\n",
    "\n",
    "X, Y = X_padded, Y_padded\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "Y = to_categorical(Y)\n",
    "Y.shape\n",
    "\n",
    "X\n",
    "X.shape\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TEST_SIZE = 0.15\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=TEST_SIZE, random_state=4)\n",
    "X_train.shape\n",
    "\n",
    "VALID_SIZE = 0.15\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X_train, Y_train, test_size=VALID_SIZE, random_state=4)\n",
    "\n",
    "print(\"TRAINING DATA\")\n",
    "print('Shape of input sequences: {}'.format(X_train.shape))\n",
    "print('Shape of output sequences: {}'.format(Y_train.shape))\n",
    "print(\"-\"*50)\n",
    "print(\"VALIDATION DATA\")\n",
    "print('Shape of input sequences: {}'.format(X_validation.shape))\n",
    "print('Shape of output sequences: {}'.format(Y_validation.shape))\n",
    "print(\"-\"*50)\n",
    "print(\"TESTING DATA\")\n",
    "print('Shape of input sequences: {}'.format(X_test.shape))\n",
    "print('Shape of output sequences: {}'.format(Y_test.shape))\n",
    "\n",
    "NUM_CLASSES = Y.shape[2]\n",
    "\n",
    "NUM_CLASSES\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "path = '/home/sentinal/Music/Folder/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "EMBEDDING_SIZE  = 300  \n",
    "VOCABULARY_SIZE = len(word_tokenizer.word_index) + 1\n",
    "\n",
    "embedding_weights = np.zeros((VOCABULARY_SIZE, EMBEDDING_SIZE))\n",
    "\n",
    "word2id = word_tokenizer.word_index\n",
    "\n",
    "for word, index in word2id.items():\n",
    "    try:\n",
    "        embedding_weights[index, :] = word2vec[word]\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "print(\"Embeddings shape: {}\".format(embedding_weights.shape))\n",
    "\n",
    "embedding_weights[word_tokenizer.word_index['joy']]\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# create architecture\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(input_dim     = VOCABULARY_SIZE,        \n",
    "                         output_dim    = EMBEDDING_SIZE,         \n",
    "                         input_length  = MAX_SEQ_LENGTH,         \n",
    "                         weights       = [embedding_weights],    \n",
    "                         trainable     = True                    \n",
    "))\n",
    "model.add(tf.keras.layers.LSTM(64, return_sequences=True))\n",
    "model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
